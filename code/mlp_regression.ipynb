{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MRC0e0KhQ0S"
      },
      "source": [
        "# Deep Learning regression of biodiversity data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWd1UlMnhT2s"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvGPUQaHhXfL"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1VMqkGvhc3-"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/amarmeddahi/sounds-of-nature.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFqwgZPpae5m",
        "outputId": "d6a1b4ca-0147-4574-b873-e2c976a8fa41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sounds-of-nature'...\n",
            "remote: Enumerating objects: 131, done.\u001b[K\n",
            "remote: Counting objects: 100% (131/131), done.\u001b[K\n",
            "remote: Compressing objects: 100% (109/109), done.\u001b[K\n",
            "remote: Total 131 (delta 45), reused 83 (delta 17), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (131/131), 33.27 MiB | 15.62 MiB/s, done.\n",
            "Resolving deltas: 100% (45/45), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M52QDmyzhh9s"
      },
      "source": [
        "path_data = \"./sounds-of-nature/data/latent_space_mel256_512.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(path_data)\n",
        "joint_table = pd.read_csv(\"./sounds-of-nature/data/reg-ind_joint_table.csv\")\n",
        "regression = pd.read_csv(\"./sounds-of-nature/data/regression.csv\")"
      ],
      "metadata": {
        "id": "jGBgfdHboUgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning the dataset"
      ],
      "metadata": {
        "id": "3fC0MepOLi2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop duplicate rows\n",
        "data = data.drop_duplicates()"
      ],
      "metadata": {
        "id": "GXCaS9T8N9oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns with high percentage of missing values\n",
        "missing_threshold = 0.8\n",
        "data = data.dropna(thresh=int(missing_threshold * len(data)), axis=1)"
      ],
      "metadata": {
        "id": "UyjhErSYPU9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing values\n",
        "data = data.dropna()"
      ],
      "metadata": {
        "id": "AZ-NRfjnRI8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting the labels"
      ],
      "metadata": {
        "id": "-roj7Jmqx4Ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.iloc[:, 3:-1].values.astype(float)"
      ],
      "metadata": {
        "id": "KhVLyqiwPZf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = []\n",
        "for filename in data['filename']:\n",
        "  id = joint_table.loc[joint_table['filename'] == filename, 'transect'].values[0]\n",
        "  y.append(regression.loc[regression['id'] == id].values[0][1:])\n",
        "y = np.array(y)"
      ],
      "metadata": {
        "id": "t4qDLPWpkL-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding the labels"
      ],
      "metadata": {
        "id": "8sAVdFGgUiPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "cat_idx = regression.columns.get_loc('TFV_G11_GROUP') - 1\n",
        "y[:,cat_idx] = le.fit_transform(y[:,cat_idx])\n",
        "y = y.astype(float)"
      ],
      "metadata": {
        "id": "3twvs8b0UnWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the categorical values and their corresponding integer values\n",
        "print('Categorical values:', le.classes_)\n",
        "print('Integer values:', le.transform(le.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeWACeiCoumZ",
        "outputId": "2fad1416-3e1c-46aa-f69e-371ce0c032e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical values: ['Conif√®res' 'Feuillus' 'Mixte']\n",
            "Integer values: [0 1 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvxIPVyMhmKp"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVzJWAXIhxoC"
      },
      "source": [
        "# Splitting randomy the dataset into the Training set and Test set (80/20)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature scaling"
      ],
      "metadata": {
        "id": "_Upa_vHS6Qb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "metadata": {
        "id": "F4RSlN596SLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the model"
      ],
      "metadata": {
        "id": "6sJJnAlZqe34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "numLabels = y_train.shape[1]\n",
        "\n",
        "# Initialising the ANN\n",
        "regressor = Sequential()\n",
        "\n",
        "# Adding the input layer and the first hidden layer\n",
        "regressor.add(Dense(units = 256, kernel_initializer = 'uniform', activation = 'relu', input_dim = X_train.shape[1]))\n",
        "regressor.add(Dense(units = 128, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "regressor.add(Dense(units = 64, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "regressor.add(Dense(units = numLabels, kernel_initializer = 'uniform', activation = 'linear'))"
      ],
      "metadata": {
        "id": "ns4Sz0djqg-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.build(input_shape=(None, X_train.shape[1]))\n",
        "regressor.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFax25hxqyuO",
        "outputId": "47c8d0a5-1137-4e78-840a-44871890907e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_4 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 14)                910       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 173,390\n",
            "Trainable params: 173,390\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')"
      ],
      "metadata": {
        "id": "MczkQNbbq3-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the models"
      ],
      "metadata": {
        "id": "at0qSGpzfcl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkpoint Callback\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "filepath=\"weights.best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "metadata": {
        "id": "TkzuyUhNi4yC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the ANN to the Training set\n",
        "history = regressor.fit(X_train, y_train, batch_size = 32, epochs = 100, validation_data=(X_test, y_test), callbacks=callbacks_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9OQWwk7q_NJ",
        "outputId": "4c6d525d-fbde-4753-c974-23a4cfe6081b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 113483.9219\n",
            "Epoch 1: val_loss improved from inf to 10595.49805, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 2s 8ms/step - loss: 109069.9922 - val_loss: 10595.4980\n",
            "Epoch 2/100\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 6824.8345\n",
            "Epoch 2: val_loss improved from 10595.49805 to 5321.15381, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 6708.1265 - val_loss: 5321.1538\n",
            "Epoch 3/100\n",
            "84/93 [==========================>...] - ETA: 0s - loss: 4251.8994\n",
            "Epoch 3: val_loss improved from 5321.15381 to 4382.03125, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 4280.1611 - val_loss: 4382.0312\n",
            "Epoch 4/100\n",
            "85/93 [==========================>...] - ETA: 0s - loss: 3360.2085\n",
            "Epoch 4: val_loss improved from 4382.03125 to 3602.60181, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 3369.6296 - val_loss: 3602.6018\n",
            "Epoch 5/100\n",
            "87/93 [===========================>..] - ETA: 0s - loss: 2830.5627\n",
            "Epoch 5: val_loss improved from 3602.60181 to 3386.65820, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 2818.7153 - val_loss: 3386.6582\n",
            "Epoch 6/100\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 2521.8394\n",
            "Epoch 6: val_loss improved from 3386.65820 to 3160.25562, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 2541.0923 - val_loss: 3160.2556\n",
            "Epoch 7/100\n",
            "83/93 [=========================>....] - ETA: 0s - loss: 2352.8069\n",
            "Epoch 7: val_loss improved from 3160.25562 to 2851.01270, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 2322.6538 - val_loss: 2851.0127\n",
            "Epoch 8/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 2074.7532\n",
            "Epoch 8: val_loss improved from 2851.01270 to 2822.43384, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 2075.5837 - val_loss: 2822.4338\n",
            "Epoch 9/100\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 1927.9363\n",
            "Epoch 9: val_loss improved from 2822.43384 to 2580.89233, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 1922.7517 - val_loss: 2580.8923\n",
            "Epoch 10/100\n",
            "82/93 [=========================>....] - ETA: 0s - loss: 1834.6318\n",
            "Epoch 10: val_loss did not improve from 2580.89233\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 1832.2394 - val_loss: 2616.5320\n",
            "Epoch 11/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 1702.9581\n",
            "Epoch 11: val_loss did not improve from 2580.89233\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 1702.9581 - val_loss: 2650.4285\n",
            "Epoch 12/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 1644.6385\n",
            "Epoch 12: val_loss improved from 2580.89233 to 2461.16040, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 1652.5669 - val_loss: 2461.1604\n",
            "Epoch 13/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 1545.3666\n",
            "Epoch 13: val_loss did not improve from 2461.16040\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 1549.0527 - val_loss: 2491.1926\n",
            "Epoch 14/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 1517.4740\n",
            "Epoch 14: val_loss improved from 2461.16040 to 2447.70020, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 1517.4740 - val_loss: 2447.7002\n",
            "Epoch 15/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 1476.8257\n",
            "Epoch 15: val_loss improved from 2447.70020 to 2383.18237, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 1476.1388 - val_loss: 2383.1824\n",
            "Epoch 16/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 1410.7014\n",
            "Epoch 16: val_loss improved from 2383.18237 to 2297.50049, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 1414.5175 - val_loss: 2297.5005\n",
            "Epoch 17/100\n",
            "85/93 [==========================>...] - ETA: 0s - loss: 1341.6888\n",
            "Epoch 17: val_loss did not improve from 2297.50049\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 1342.2212 - val_loss: 2343.5654\n",
            "Epoch 18/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 1300.2377\n",
            "Epoch 18: val_loss did not improve from 2297.50049\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 1298.3883 - val_loss: 2431.7881\n",
            "Epoch 19/100\n",
            "81/93 [=========================>....] - ETA: 0s - loss: 1250.7168\n",
            "Epoch 19: val_loss improved from 2297.50049 to 2288.85547, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 1259.9677 - val_loss: 2288.8555\n",
            "Epoch 20/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 1219.7025\n",
            "Epoch 20: val_loss did not improve from 2288.85547\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 1219.7025 - val_loss: 2381.9131\n",
            "Epoch 21/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 1153.8610\n",
            "Epoch 21: val_loss improved from 2288.85547 to 2241.50366, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 1156.2750 - val_loss: 2241.5037\n",
            "Epoch 22/100\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 1158.7924\n",
            "Epoch 22: val_loss improved from 2241.50366 to 2220.79883, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 1160.2013 - val_loss: 2220.7988\n",
            "Epoch 23/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 1145.5088\n",
            "Epoch 23: val_loss improved from 2220.79883 to 2203.92578, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 1150.2032 - val_loss: 2203.9258\n",
            "Epoch 24/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 1094.4354\n",
            "Epoch 24: val_loss did not improve from 2203.92578\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 1094.6885 - val_loss: 2206.7422\n",
            "Epoch 25/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 1056.2545\n",
            "Epoch 25: val_loss improved from 2203.92578 to 2181.10083, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 1054.7310 - val_loss: 2181.1008\n",
            "Epoch 26/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 1018.9904\n",
            "Epoch 26: val_loss improved from 2181.10083 to 2118.62305, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 1018.9904 - val_loss: 2118.6230\n",
            "Epoch 27/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 1043.9916\n",
            "Epoch 27: val_loss improved from 2118.62305 to 2069.80737, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 1043.1602 - val_loss: 2069.8074\n",
            "Epoch 28/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 1010.1428\n",
            "Epoch 28: val_loss did not improve from 2069.80737\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 1012.6337 - val_loss: 2070.4077\n",
            "Epoch 29/100\n",
            "86/93 [==========================>...] - ETA: 0s - loss: 1016.0283\n",
            "Epoch 29: val_loss did not improve from 2069.80737\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 1009.2798 - val_loss: 2124.2937\n",
            "Epoch 30/100\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 957.8154\n",
            "Epoch 30: val_loss did not improve from 2069.80737\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 965.8887 - val_loss: 2199.8887\n",
            "Epoch 31/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 974.2263\n",
            "Epoch 31: val_loss improved from 2069.80737 to 2050.63428, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 974.2263 - val_loss: 2050.6343\n",
            "Epoch 32/100\n",
            "82/93 [=========================>....] - ETA: 0s - loss: 929.1315\n",
            "Epoch 32: val_loss did not improve from 2050.63428\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 928.3886 - val_loss: 2154.0332\n",
            "Epoch 33/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 918.4622\n",
            "Epoch 33: val_loss improved from 2050.63428 to 2043.19690, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 920.7600 - val_loss: 2043.1969\n",
            "Epoch 34/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 934.8083\n",
            "Epoch 34: val_loss did not improve from 2043.19690\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 931.6113 - val_loss: 2116.5664\n",
            "Epoch 35/100\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 875.0833\n",
            "Epoch 35: val_loss did not improve from 2043.19690\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 890.0096 - val_loss: 2141.5752\n",
            "Epoch 36/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 932.6852\n",
            "Epoch 36: val_loss did not improve from 2043.19690\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 934.2753 - val_loss: 2115.2305\n",
            "Epoch 37/100\n",
            "82/93 [=========================>....] - ETA: 0s - loss: 882.7663\n",
            "Epoch 37: val_loss improved from 2043.19690 to 2036.88257, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 883.8887 - val_loss: 2036.8826\n",
            "Epoch 38/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 867.1185\n",
            "Epoch 38: val_loss did not improve from 2036.88257\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 867.1185 - val_loss: 2042.0706\n",
            "Epoch 39/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 857.9622\n",
            "Epoch 39: val_loss did not improve from 2036.88257\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 867.7684 - val_loss: 2041.1013\n",
            "Epoch 40/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 816.6344\n",
            "Epoch 40: val_loss improved from 2036.88257 to 1967.10364, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 817.4368 - val_loss: 1967.1036\n",
            "Epoch 41/100\n",
            "86/93 [==========================>...] - ETA: 0s - loss: 811.8284\n",
            "Epoch 41: val_loss did not improve from 1967.10364\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 812.9752 - val_loss: 1994.9406\n",
            "Epoch 42/100\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 840.2023\n",
            "Epoch 42: val_loss improved from 1967.10364 to 1919.24988, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 843.9299 - val_loss: 1919.2499\n",
            "Epoch 43/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 811.3749\n",
            "Epoch 43: val_loss did not improve from 1919.24988\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 809.2983 - val_loss: 2010.0643\n",
            "Epoch 44/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 773.8065\n",
            "Epoch 44: val_loss did not improve from 1919.24988\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 772.8917 - val_loss: 1986.5049\n",
            "Epoch 45/100\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 735.7815\n",
            "Epoch 45: val_loss did not improve from 1919.24988\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 738.3809 - val_loss: 1998.6406\n",
            "Epoch 46/100\n",
            "85/93 [==========================>...] - ETA: 0s - loss: 736.8684\n",
            "Epoch 46: val_loss did not improve from 1919.24988\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 744.3259 - val_loss: 1955.6073\n",
            "Epoch 47/100\n",
            "82/93 [=========================>....] - ETA: 0s - loss: 725.9175\n",
            "Epoch 47: val_loss improved from 1919.24988 to 1915.01355, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 729.2239 - val_loss: 1915.0135\n",
            "Epoch 48/100\n",
            "87/93 [===========================>..] - ETA: 0s - loss: 722.2852\n",
            "Epoch 48: val_loss improved from 1915.01355 to 1882.15125, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 716.9269 - val_loss: 1882.1512\n",
            "Epoch 49/100\n",
            "86/93 [==========================>...] - ETA: 0s - loss: 708.6089\n",
            "Epoch 49: val_loss did not improve from 1882.15125\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 702.6146 - val_loss: 1923.4683\n",
            "Epoch 50/100\n",
            "84/93 [==========================>...] - ETA: 0s - loss: 666.0232\n",
            "Epoch 50: val_loss did not improve from 1882.15125\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 677.9671 - val_loss: 1920.3213\n",
            "Epoch 51/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 681.4819\n",
            "Epoch 51: val_loss did not improve from 1882.15125\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 678.3755 - val_loss: 1969.8970\n",
            "Epoch 52/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 690.8956\n",
            "Epoch 52: val_loss improved from 1882.15125 to 1878.39050, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 689.7300 - val_loss: 1878.3905\n",
            "Epoch 53/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 665.9200\n",
            "Epoch 53: val_loss improved from 1878.39050 to 1865.15527, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 665.9200 - val_loss: 1865.1553\n",
            "Epoch 54/100\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 689.7597\n",
            "Epoch 54: val_loss improved from 1865.15527 to 1859.53528, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 687.6182 - val_loss: 1859.5353\n",
            "Epoch 55/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 616.6000\n",
            "Epoch 55: val_loss improved from 1859.53528 to 1819.82776, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 618.4722 - val_loss: 1819.8278\n",
            "Epoch 56/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 599.9080\n",
            "Epoch 56: val_loss did not improve from 1819.82776\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 601.2040 - val_loss: 1828.4327\n",
            "Epoch 57/100\n",
            "85/93 [==========================>...] - ETA: 0s - loss: 558.4824\n",
            "Epoch 57: val_loss improved from 1819.82776 to 1758.45496, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 555.8986 - val_loss: 1758.4550\n",
            "Epoch 58/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 540.1614\n",
            "Epoch 58: val_loss did not improve from 1758.45496\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 540.1614 - val_loss: 1790.2026\n",
            "Epoch 59/100\n",
            "82/93 [=========================>....] - ETA: 0s - loss: 546.8496\n",
            "Epoch 59: val_loss did not improve from 1758.45496\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 546.7347 - val_loss: 1816.0178\n",
            "Epoch 60/100\n",
            "85/93 [==========================>...] - ETA: 0s - loss: 522.4164\n",
            "Epoch 60: val_loss did not improve from 1758.45496\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 521.7664 - val_loss: 1763.9188\n",
            "Epoch 61/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 503.6313\n",
            "Epoch 61: val_loss improved from 1758.45496 to 1736.37415, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 503.6313 - val_loss: 1736.3741\n",
            "Epoch 62/100\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 507.6152\n",
            "Epoch 62: val_loss did not improve from 1736.37415\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 505.7062 - val_loss: 1740.8076\n",
            "Epoch 63/100\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 478.6232\n",
            "Epoch 63: val_loss did not improve from 1736.37415\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 487.1134 - val_loss: 1742.2329\n",
            "Epoch 64/100\n",
            "85/93 [==========================>...] - ETA: 0s - loss: 469.4009\n",
            "Epoch 64: val_loss improved from 1736.37415 to 1727.94824, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 467.8982 - val_loss: 1727.9482\n",
            "Epoch 65/100\n",
            "85/93 [==========================>...] - ETA: 0s - loss: 428.9603\n",
            "Epoch 65: val_loss improved from 1727.94824 to 1716.59949, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 435.2125 - val_loss: 1716.5995\n",
            "Epoch 66/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 435.3213\n",
            "Epoch 66: val_loss improved from 1716.59949 to 1637.72192, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 437.6573 - val_loss: 1637.7219\n",
            "Epoch 67/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 416.4023\n",
            "Epoch 67: val_loss improved from 1637.72192 to 1620.34326, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 415.1521 - val_loss: 1620.3433\n",
            "Epoch 68/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 391.0583\n",
            "Epoch 68: val_loss did not improve from 1620.34326\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 390.3424 - val_loss: 1705.2970\n",
            "Epoch 69/100\n",
            "93/93 [==============================] - ETA: 0s - loss: 407.7987\n",
            "Epoch 69: val_loss improved from 1620.34326 to 1616.71606, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 407.7987 - val_loss: 1616.7161\n",
            "Epoch 70/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 357.2842\n",
            "Epoch 70: val_loss did not improve from 1616.71606\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 356.2311 - val_loss: 1649.0978\n",
            "Epoch 71/100\n",
            "86/93 [==========================>...] - ETA: 0s - loss: 354.1060\n",
            "Epoch 71: val_loss did not improve from 1616.71606\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 350.9453 - val_loss: 1655.2692\n",
            "Epoch 72/100\n",
            "84/93 [==========================>...] - ETA: 0s - loss: 320.6435\n",
            "Epoch 72: val_loss improved from 1616.71606 to 1610.29956, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 331.2669 - val_loss: 1610.2996\n",
            "Epoch 73/100\n",
            "80/93 [========================>.....] - ETA: 0s - loss: 302.3796\n",
            "Epoch 73: val_loss did not improve from 1610.29956\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 321.1747 - val_loss: 1624.8585\n",
            "Epoch 74/100\n",
            "82/93 [=========================>....] - ETA: 0s - loss: 352.4378\n",
            "Epoch 74: val_loss improved from 1610.29956 to 1606.64783, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 353.2828 - val_loss: 1606.6478\n",
            "Epoch 75/100\n",
            "90/93 [============================>.] - ETA: 0s - loss: 325.3488\n",
            "Epoch 75: val_loss did not improve from 1606.64783\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 320.7041 - val_loss: 1621.2144\n",
            "Epoch 76/100\n",
            "87/93 [===========================>..] - ETA: 0s - loss: 306.7949\n",
            "Epoch 76: val_loss did not improve from 1606.64783\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 304.3102 - val_loss: 1661.5885\n",
            "Epoch 77/100\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 324.0123\n",
            "Epoch 77: val_loss improved from 1606.64783 to 1597.49731, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 326.1136 - val_loss: 1597.4973\n",
            "Epoch 78/100\n",
            "86/93 [==========================>...] - ETA: 0s - loss: 295.9850\n",
            "Epoch 78: val_loss did not improve from 1597.49731\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 302.3781 - val_loss: 1634.0979\n",
            "Epoch 79/100\n",
            "80/93 [========================>.....] - ETA: 0s - loss: 307.5628\n",
            "Epoch 79: val_loss did not improve from 1597.49731\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 303.7346 - val_loss: 1616.5959\n",
            "Epoch 80/100\n",
            "85/93 [==========================>...] - ETA: 0s - loss: 268.5986\n",
            "Epoch 80: val_loss did not improve from 1597.49731\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 265.4044 - val_loss: 1623.7687\n",
            "Epoch 81/100\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 241.1677\n",
            "Epoch 81: val_loss did not improve from 1597.49731\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 241.3646 - val_loss: 1612.6736\n",
            "Epoch 82/100\n",
            "82/93 [=========================>....] - ETA: 0s - loss: 259.9310\n",
            "Epoch 82: val_loss did not improve from 1597.49731\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 264.0735 - val_loss: 1597.5919\n",
            "Epoch 83/100\n",
            "85/93 [==========================>...] - ETA: 0s - loss: 256.2149\n",
            "Epoch 83: val_loss did not improve from 1597.49731\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 250.3761 - val_loss: 1708.8385\n",
            "Epoch 84/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 259.3383\n",
            "Epoch 84: val_loss did not improve from 1597.49731\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 258.1589 - val_loss: 1638.6597\n",
            "Epoch 85/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 242.5974\n",
            "Epoch 85: val_loss did not improve from 1597.49731\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 241.1008 - val_loss: 1647.7914\n",
            "Epoch 86/100\n",
            "83/93 [=========================>....] - ETA: 0s - loss: 278.5464\n",
            "Epoch 86: val_loss did not improve from 1597.49731\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 274.0222 - val_loss: 1757.6973\n",
            "Epoch 87/100\n",
            "85/93 [==========================>...] - ETA: 0s - loss: 261.5953\n",
            "Epoch 87: val_loss did not improve from 1597.49731\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 254.7040 - val_loss: 1621.3545\n",
            "Epoch 88/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 234.2453\n",
            "Epoch 88: val_loss did not improve from 1597.49731\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 232.6537 - val_loss: 1660.4449\n",
            "Epoch 89/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 240.0265\n",
            "Epoch 89: val_loss did not improve from 1597.49731\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 238.6355 - val_loss: 1617.1129\n",
            "Epoch 90/100\n",
            "87/93 [===========================>..] - ETA: 0s - loss: 224.7188\n",
            "Epoch 90: val_loss improved from 1597.49731 to 1587.10046, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 225.0622 - val_loss: 1587.1005\n",
            "Epoch 91/100\n",
            "83/93 [=========================>....] - ETA: 0s - loss: 206.9461\n",
            "Epoch 91: val_loss did not improve from 1587.10046\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 220.5699 - val_loss: 1645.0394\n",
            "Epoch 92/100\n",
            "82/93 [=========================>....] - ETA: 0s - loss: 281.7260\n",
            "Epoch 92: val_loss did not improve from 1587.10046\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 273.6467 - val_loss: 1598.6447\n",
            "Epoch 93/100\n",
            "83/93 [=========================>....] - ETA: 0s - loss: 219.6856\n",
            "Epoch 93: val_loss did not improve from 1587.10046\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 210.4142 - val_loss: 1609.0876\n",
            "Epoch 94/100\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 226.3358\n",
            "Epoch 94: val_loss improved from 1587.10046 to 1570.23755, saving model to weights.best.hdf5\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 223.2005 - val_loss: 1570.2375\n",
            "Epoch 95/100\n",
            "87/93 [===========================>..] - ETA: 0s - loss: 190.3223\n",
            "Epoch 95: val_loss did not improve from 1570.23755\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 200.4648 - val_loss: 1618.6655\n",
            "Epoch 96/100\n",
            "92/93 [============================>.] - ETA: 0s - loss: 249.0768\n",
            "Epoch 96: val_loss did not improve from 1570.23755\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 247.6768 - val_loss: 1669.7202\n",
            "Epoch 97/100\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 217.6741\n",
            "Epoch 97: val_loss did not improve from 1570.23755\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 215.6796 - val_loss: 1618.9437\n",
            "Epoch 98/100\n",
            "87/93 [===========================>..] - ETA: 0s - loss: 202.0968\n",
            "Epoch 98: val_loss did not improve from 1570.23755\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 207.4561 - val_loss: 1652.4446\n",
            "Epoch 99/100\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 228.1605\n",
            "Epoch 99: val_loss did not improve from 1570.23755\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 227.2635 - val_loss: 1717.1017\n",
            "Epoch 100/100\n",
            "91/93 [============================>.] - ETA: 0s - loss: 209.8726\n",
            "Epoch 100: val_loss did not improve from 1570.23755\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 208.3682 - val_loss: 1654.7211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation accuracy and loss values in the same figure (1 row, 2 columns)\n",
        "fig, ax = plt.subplots(figsize=(10, 5)) \n",
        "ax.plot(history.history['loss'], label='Training loss')\n",
        "ax.plot(history.history['val_loss'], label='Validation loss')\n",
        "ax.set_title('Model loss')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.legend(loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "KpQSJcZtr0c9",
        "outputId": "dbdc9b8c-5904-42ec-fdc3-ffff155b83a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "regressor.load_weights(filepath)"
      ],
      "metadata": {
        "id": "ofKpWlaVr3bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the model"
      ],
      "metadata": {
        "id": "wrMzqAzSxbmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "score = regressor.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1Yecxw5r6Jh",
        "outputId": "4089bf33-fbe7-4de0-cde2-0957d8e077e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1570.237548828125\n"
          ]
        }
      ]
    }
  ]
}